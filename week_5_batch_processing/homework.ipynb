{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Run the command \"spark.version\" What version number is output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Ensure that pyspark is available and that the environment variables are set\n",
    "findspark.init()\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.version"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Read it with Spark using the same schema as we did in the lessons.\n",
    "We will use this dataset for all the remaining questions.\n",
    "Repartition it to 12 partitions and save it to parquet.\n",
    "What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)? Select the answer which most closely matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data\n",
    "!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-06.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dispatching_base_num', 'string'),\n",
       " ('pickup_datetime', 'string'),\n",
       " ('dropoff_datetime', 'string'),\n",
       " ('PULocationID', 'string'),\n",
       " ('DOLocationID', 'string'),\n",
       " ('SR_Flag', 'string'),\n",
       " ('Affiliated_base_number', 'string')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the column names and default types - IMPORTANT\n",
    "df = spark.read.option('header', \"True\").csv('fhvhv_tripdata_2021-06.csv.gz')\n",
    "df.limit(5).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an appropriate schema for the dataframe\n",
    "schema = types.StructType([\n",
    "    types.StructField('dispatching_base_num',types.StringType(),True),\n",
    "    types.StructField('pickup_datetime',types.TimestampType(),True),\n",
    "    types.StructField('dropoff_datetime',types.TimestampType(),True),\n",
    "    types.StructField('PULocationID',types.IntegerType(),True),\n",
    "    types.StructField('DOLocationID',types.IntegerType(),True),\n",
    "    types.StructField('SR_Flag',types.StringType(),True),\n",
    "    types.StructField('Affiliated_base_number',types.StringType(),True),\n",
    "])\n",
    "\n",
    "# Read CSV data and write it in 12 partitions in PARQUET format\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"True\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('fhvhv_tripdata_2021-06.csv.gz')\n",
    "df.repartition(12).write.parquet('fhvhv/2021/06/') \n",
    "# ANSWER: 24 MB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Count records\n",
    "\n",
    "How many taxi trips were there on June 15?\n",
    "\n",
    "Consider only trips that started on June 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "452470"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create date column\n",
    "df = df.withColumn('pickup_date', F.to_date(df.pickup_datetime))\n",
    "df = df.withColumn('dropoff_date', F.to_date(df.dropoff_datetime))\n",
    "df.filter(\"pickup_date = '2021-06-15'\").count()\n",
    "# ANSWER: 452470\n",
    "# df.filter((F.col(\"pickup_date\") == '2021-06-15') & (F.col(\"dropoff_date\") == '2021-06-15')).count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "Longest trip for each day\n",
    "\n",
    "Now calculate the duration for each trip.\n",
    "How long was the longest trip in Hours?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|pickup_date|     max(duration)|\n",
      "+-----------+------------------+\n",
      "| 2021-06-25|  66.8788888888889|\n",
      "| 2021-06-22|25.549722222222222|\n",
      "| 2021-06-27|19.980833333333333|\n",
      "| 2021-06-26|18.197222222222223|\n",
      "| 2021-06-23|16.466944444444444|\n",
      "| 2021-06-24|13.909722222222221|\n",
      "| 2021-06-04|             11.67|\n",
      "| 2021-06-20|10.984444444444444|\n",
      "| 2021-06-01|           10.2675|\n",
      "| 2021-06-28| 9.966388888888888|\n",
      "| 2021-06-18| 9.624444444444444|\n",
      "| 2021-06-08| 9.480277777777777|\n",
      "| 2021-06-11| 9.471666666666666|\n",
      "| 2021-06-15| 9.402222222222223|\n",
      "| 2021-06-03| 9.365833333333333|\n",
      "| 2021-06-19| 9.106944444444444|\n",
      "| 2021-06-30| 9.056111111111111|\n",
      "| 2021-06-09| 9.030277777777778|\n",
      "| 2021-06-17| 8.774166666666666|\n",
      "| 2021-06-29| 8.571666666666667|\n",
      "+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, col\n",
    "# Convert the pickup and dropoff timestamps to Unix timestamps:\n",
    "df = df.withColumn('pickup_timestamp', unix_timestamp(col('pickup_datetime')))\n",
    "df = df.withColumn('dropoff_timestamp', unix_timestamp(col('dropoff_datetime')))\n",
    "# Estimate duration of each trip in seconds\n",
    "df = df.withColumn('duration', (col('dropoff_timestamp') - col('pickup_timestamp')) / 3600.0)\n",
    "# Estimate the maximum duration for each date\n",
    "df.groupBy('pickup_date') \\\n",
    "    .agg({'duration': 'max'}) \\\n",
    "    .orderBy('max(duration)', ascending=False) \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|pickup_date|          duration|\n",
      "+-----------+------------------+\n",
      "| 2021-06-25|  66.8788888888889|\n",
      "| 2021-06-22|25.549722222222222|\n",
      "| 2021-06-27|19.980833333333333|\n",
      "| 2021-06-26|18.197222222222223|\n",
      "| 2021-06-23|16.466944444444444|\n",
      "| 2021-06-24|13.909722222222221|\n",
      "| 2021-06-04|             11.67|\n",
      "| 2021-06-20|10.984444444444444|\n",
      "| 2021-06-01|           10.2675|\n",
      "| 2021-06-28| 9.966388888888888|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Another way\n",
    "df.registerTempTable('fhvhv_2021_06')\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    to_date(pickup_datetime) AS pickup_date,\n",
    "    MAX((CAST(dropoff_datetime AS LONG) - CAST(pickup_datetime AS LONG)) / 3600) AS duration\n",
    "FROM fhvhv_2021_06\n",
    "GROUP BY 1\n",
    "ORDER BY 2 DESC\n",
    "LIMIT 10;\n",
    "\"\"\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "Sparkâ€™s User Interface which shows application's dashboard runs on which local port?\n",
    "ANSWER: 4040"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "Load the zone lookup data into a temp view in Spark\n",
    "Zone Data\n",
    "\n",
    "Using the zone lookup data and the fhvhv June 2021 data, what is the name of the most frequent pickup location zone?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|                Zone| trips|\n",
      "+--------------------+------+\n",
      "| Crown Heights North|231279|\n",
      "|        East Village|221244|\n",
      "|         JFK Airport|188867|\n",
      "|      Bushwick South|187929|\n",
      "|       East New York|186780|\n",
      "|TriBeCa/Civic Center|164344|\n",
      "|   LaGuardia Airport|161596|\n",
      "|            Union Sq|158937|\n",
      "|        West Village|154698|\n",
      "|             Astoria|152493|\n",
      "|     Lower East Side|151020|\n",
      "|        East Chelsea|147673|\n",
      "|Central Harlem North|146402|\n",
      "|Williamsburg (Nor...|143683|\n",
      "|          Park Slope|143594|\n",
      "|  Stuyvesant Heights|141427|\n",
      "|        Clinton East|139611|\n",
      "|West Chelsea/Huds...|139431|\n",
      "|             Bedford|138428|\n",
      "|         Murray Hill|137879|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read zones dataframe\n",
    "df_zones = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('taxi_zone_lookup.csv')\n",
    "df_zones.registerTempTable('df_zones')\n",
    "# Count trips per each pick up location\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    pul.Zone,\n",
    "    count(1) as trips\n",
    "FROM fhvhv_2021_06 AS fhv \n",
    "    LEFT JOIN df_zones AS pul ON fhv.PULocationID = pul.LocationID\n",
    "GROUP BY 1\n",
    "ORDER BY 2 DESC\n",
    "\"\"\").show()\n",
    "# ANSWER: Crown Heights North"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de-zoomcamp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
